{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7f38be-91aa-4536-ae85-fb0f8f504611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# set the experiment id\n",
    "mlflow.set_experiment(experiment_id=\"1104210664176753\")\n",
    "\n",
    "mlflow.autolog()\n",
    "db = load_diabetes()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
    "\n",
    "# Create and train models.\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to make predictions on the test dataset.\n",
    "predictions = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18094e00-a735-436f-83f0-0fd7a813dcd4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports and Utilities"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Utilities\n",
    "# Purpose: Import essential libraries for Spark, LightGBM, sklearn, matplotlib, and seaborn.\n",
    "# Note: The one_hot_encode function is commented out and not used in the current pipeline. It was removed for simplicity and performance.\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "# def one_hot_encode(df, input_col, output_col_prefix):\n",
    "#     indexer = StringIndexer(inputCol=input_col, outputCol=f\"{input_col}_index\")\n",
    "#     encoder = OneHotEncoder(inputCol=f\"{input_col}_index\", outputCol=f\"{output_col_prefix}_vec\")\n",
    "#     pipeline = Pipeline(stages=[indexer, encoder])\n",
    "#     model = pipeline.fit(df)\n",
    "#     encoded_df = model.transform(df)\n",
    "#     return encoded_df\n",
    "\n",
    "# Example usage:\n",
    "# encoded_df = one_hot_encode(df, \"category_column\", \"category\")\n",
    "# display(encoded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e704068f-6eb3-47b9-a752-4b8eaea9e984",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Timer Context Manager"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Timer Context Manager\n",
    "# Purpose: Defines a timer context manager for profiling code execution, useful for measuring the runtime of pipeline steps.\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import gc\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f29eba-ed32-4eb1-afc5-489566d66f92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Application Data Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Application Data Feature Engineering\n",
    "# Purpose: Loads training and test data from Spark tables, processes missing values, creates new features, and merges train/test sets for modeling.\n",
    "# Note: Spark one-hot encoding for binary features is commented out and not used. Numeric-only pipeline for simplicity and performance.\n",
    "def application_train_test(num_rows=None, nan_as_category=True):\n",
    "    df = spark.table(\"workspace.silver_creditrisk.app_train\")\n",
    "    test_df = spark.table(\"workspace.silver_creditrisk.app_test\")\n",
    "    if num_rows:\n",
    "        df = df.limit(num_rows)\n",
    "        test_df = test_df.limit(num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(df.count(), test_df.count()))\n",
    "    test_df = test_df.withColumn(\"TARGET\", F.lit(None))\n",
    "    df = df.unionByName(test_df)\n",
    "    # Remove Spark one-hot encoding\n",
    "    # for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "    #     df = one_hot_encode(df, bin_feature, bin_feature)\n",
    "    df = df.withColumn('DAYS_EMPLOYED', F.when(df['DAYS_EMPLOYED'] == 365243, None).otherwise(df['DAYS_EMPLOYED']))\n",
    "    df = df.withColumn('DAYS_EMPLOYED_PERC', F.try_divide(df['DAYS_EMPLOYED'], df['DAYS_BIRTH'])) \\\n",
    "           .withColumn('INCOME_CREDIT_PERC', F.try_divide(df['AMT_INCOME_TOTAL'], df['AMT_CREDIT'])) \\\n",
    "           .withColumn('INCOME_PER_PERSON', F.try_divide(df['AMT_INCOME_TOTAL'], df['CNT_FAM_MEMBERS'])) \\\n",
    "           .withColumn('ANNUITY_INCOME_PERC', F.try_divide(df['AMT_ANNUITY'], df['AMT_INCOME_TOTAL']))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f078938-f1f3-4846-a934-3611cfa1ad14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bureau and Bureau Balance Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Bureau and Bureau Balance Feature Engineering\n",
    "# Purpose: Aggregates bureau and bureau_balance tables, engineers features via groupBy and aggregation, and joins results for downstream use.\n",
    "# Note: Spark one-hot encoding and categorical feature aggregation are commented out and not used. Numeric-only pipeline for simplicity and performance.\n",
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = spark.table(\"workspace.silver_creditrisk.bureau\")\n",
    "    bb = spark.table(\"workspace.silver_creditrisk.bureau_balance\")\n",
    "    if num_rows:\n",
    "        bureau = bureau.limit(num_rows)\n",
    "        bb = bb.limit(num_rows)\n",
    "    # Remove Spark one-hot encoding\n",
    "    # bb = one_hot_encode(bb, \"STATUS\", \"STATUS\")\n",
    "    # bureau = one_hot_encode(bureau, \"CREDIT_ACTIVE\", \"CREDIT_ACTIVE\")\n",
    "    bb_agg_exprs = [\n",
    "        F.min(\"MONTHS_BALANCE\").alias(\"MONTHS_BALANCE_MIN\"),\n",
    "        F.max(\"MONTHS_BALANCE\").alias(\"MONTHS_BALANCE_MAX\"),\n",
    "        F.count(\"MONTHS_BALANCE\").alias(\"MONTHS_BALANCE_SIZE\"),\n",
    "    ]\n",
    "    bb_agg = bb.groupBy(\"SK_ID_BUREAU\").agg(*bb_agg_exprs)\n",
    "    bureau = bureau.join(bb_agg, on=\"SK_ID_BUREAU\", how=\"left\")\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': [F.min, F.max, F.mean, F.variance],\n",
    "        'CREDIT_DAY_OVERDUE': [F.max, F.mean],\n",
    "        'DAYS_CREDIT_ENDDATE': [F.min, F.max, F.mean],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': [F.mean],\n",
    "        'CNT_CREDIT_PROLONG': [F.sum],\n",
    "        'AMT_CREDIT_SUM': [F.max, F.mean, F.sum],\n",
    "        'AMT_CREDIT_SUM_DEBT': [F.max, F.mean, F.sum],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': [F.mean],\n",
    "        'AMT_CREDIT_SUM_LIMIT': [F.mean, F.sum],\n",
    "        'DAYS_CREDIT_UPDATE': [F.min, F.max, F.mean],\n",
    "        'AMT_ANNUITY': [F.max, F.mean],\n",
    "        'MONTHS_BALANCE_MIN': [F.min],\n",
    "        'MONTHS_BALANCE_MAX': [F.max],\n",
    "        'MONTHS_BALANCE_SIZE': [F.mean, F.sum]\n",
    "    }\n",
    "    agg_exprs = []\n",
    "    for col, funcs in num_aggregations.items():\n",
    "        for func in funcs:\n",
    "            agg_exprs.append(func(col).alias(f\"BURO_{col}_{func.__name__.upper()}\"))\n",
    "    # Remove categorical feature aggregation\n",
    "    # cat_features = ['CREDIT_ACTIVE_vec', 'CREDIT_CURRENCY_vec', 'CREDIT_TYPE_vec']\n",
    "    # for cat in cat_features:\n",
    "    #     agg_exprs.append(F.mean(cat).alias(f\"BURO_{cat}_MEAN\"))\n",
    "    bureau_agg = bureau.groupBy(\"SK_ID_CURR\").agg(*agg_exprs)\n",
    "    # Remove active and closed credit aggregations that use CREDIT_ACTIVE_vec[1] and [2]\n",
    "    # active = bureau.filter(F.col('CREDIT_ACTIVE_vec')[1] == 1)\n",
    "    # active_agg = active.groupBy(\"SK_ID_CURR\").agg(*[\n",
    "    #     func(col).alias(f\"ACT_{col}_{func.__name__.upper()}\")\n",
    "    #     for col, funcs in num_aggregations.items() for func in funcs\n",
    "    # ])\n",
    "    # bureau_agg = bureau_agg.join(active_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    # closed = bureau.filter(F.col('CREDIT_ACTIVE_vec')[2] == 1)\n",
    "    # closed_agg = closed.groupBy(\"SK_ID_CURR\").agg(*[\n",
    "    #     func(col).alias(f\"CLS_{col}_{func.__name__.upper()}\")\n",
    "    #     for col, funcs in num_aggregations.items() for func in funcs\n",
    "    # ])\n",
    "    # bureau_agg = bureau_agg.join(closed_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    return bureau_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ee2501-7998-4772-b6e3-59ae60c17e2a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Previous Applications Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Previous Applications Feature Engineering\n",
    "# Purpose: Processes previous loan applications, handles missing values, engineers features, and aggregates by customer ID.\n",
    "# Note: Spark one-hot encoding and categorical aggregations are commented out and not used. Numeric-only pipeline for simplicity and performance.\n",
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    prev = spark.table(\"workspace.silver_creditrisk.previous_application\")\n",
    "    if num_rows:\n",
    "        prev = prev.limit(num_rows)\n",
    "    # prev = one_hot_encode(prev, \"NAME_CONTRACT_STATUS\", \"NAME_CONTRACT_STATUS\")\n",
    "    #365243 is a place holder for missing values\n",
    "    prev = prev.withColumn('DAYS_FIRST_DRAWING', F.when(prev['DAYS_FIRST_DRAWING'] == 365243, None).otherwise(prev['DAYS_FIRST_DRAWING']))\n",
    "    prev = prev.withColumn('DAYS_FIRST_DUE', F.when(prev['DAYS_FIRST_DUE'] == 365243, None).otherwise(prev['DAYS_FIRST_DUE']))\n",
    "    prev = prev.withColumn('DAYS_LAST_DUE_1ST_VERSION', F.when(prev['DAYS_LAST_DUE_1ST_VERSION'] == 365243, None).otherwise(prev['DAYS_LAST_DUE_1ST_VERSION']))\n",
    "    prev = prev.withColumn('DAYS_LAST_DUE', F.when(prev['DAYS_LAST_DUE'] == 365243, None).otherwise(prev['DAYS_LAST_DUE']))\n",
    "    prev = prev.withColumn('DAYS_TERMINATION', F.when(prev['DAYS_TERMINATION'] == 365243, None).otherwise(prev['DAYS_TERMINATION']))\n",
    "    # Update division to use F.try_divide\n",
    "    prev = prev.withColumn('APP_CREDIT_PERC', F.try_divide(prev['AMT_APPLICATION'], prev['AMT_CREDIT']))\n",
    "\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': [F.min, F.max, F.mean],\n",
    "        'AMT_APPLICATION': [F.min, F.max, F.mean],\n",
    "        'AMT_CREDIT': [F.min, F.max, F.mean],\n",
    "        'APP_CREDIT_PERC': [F.min, F.max, F.mean, F.variance],\n",
    "        'AMT_DOWN_PAYMENT': [F.min, F.max, F.mean],\n",
    "        'AMT_GOODS_PRICE': [F.min, F.max, F.mean],\n",
    "        'HOUR_APPR_PROCESS_START': [F.min, F.max, F.mean],\n",
    "        'RATE_DOWN_PAYMENT': [F.min, F.max, F.mean],\n",
    "        'DAYS_DECISION': [F.min, F.max, F.mean],\n",
    "        'CNT_PAYMENT': [F.mean, F.sum],\n",
    "    }\n",
    "\n",
    "    agg_exprs = []\n",
    "    for col, funcs in num_aggregations.items():\n",
    "        for func in funcs:\n",
    "            agg_exprs.append(func(col).alias(f\"PREV_{col}_{func.__name__.upper()}\"))\n",
    "\n",
    "    # cat_features = [c for c in prev.columns if c.startswith(\"NAME_CONTRACT_STATUS_vec\")]\n",
    "    # for cat in cat_features:\n",
    "    #     agg_exprs.append(F.mean(cat).alias(f\"PREV_{cat}_MEAN\"))\n",
    "\n",
    "    prev_agg = prev.groupBy(\"SK_ID_CURR\").agg(*agg_exprs)\n",
    "\n",
    "    # approved = prev.filter(F.col('NAME_CONTRACT_STATUS_vec')[1] == 1)\n",
    "    # approved_agg = approved.groupBy(\"SK_ID_CURR\").agg(*[\n",
    "    #     func(col).alias(f\"APR_{col}_{func.__name__.upper()}\")\n",
    "    #     for col, funcs in num_aggregations.items() for func in funcs\n",
    "    # ])\n",
    "    # prev_agg = prev_agg.join(approved_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "\n",
    "    # refused = prev.filter(F.col('NAME_CONTRACT_STATUS_vec')[2] == 1)\n",
    "    # refused_agg = refused.groupBy(\"SK_ID_CURR\").agg(*[\n",
    "    #     func(col).alias(f\"REF_{col}_{func.__name__.upper()}\")\n",
    "    #     for col, funcs in num_aggregations.items() for func in funcs\n",
    "    # ])\n",
    "    # prev_agg = prev_agg.join(refused_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "\n",
    "    return prev_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d910fa6-098c-41fe-9173-bd6b4010df44",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "POS Cash Balance Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: POS Cash Balance Feature Engineering\n",
    "# Purpose: Processes POS cash balance data, aggregates time and delinquency features, and prepares them for joining.\n",
    "# Note: Spark one-hot encoding and categorical aggregations are commented out and not used. Numeric-only pipeline for simplicity and performance.\n",
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = spark.table(\"workspace.silver_creditrisk.pos_cash_balance\")\n",
    "    if num_rows:\n",
    "        pos = pos.limit(num_rows)\n",
    "    # pos = one_hot_encode(pos, \"NAME_CONTRACT_STATUS\", \"NAME_CONTRACT_STATUS\")\n",
    "    agg_exprs = [\n",
    "        F.max(\"MONTHS_BALANCE\").alias(\"POS_MONTHS_BALANCE_MAX\"),\n",
    "        F.mean(\"MONTHS_BALANCE\").alias(\"POS_MONTHS_BALANCE_MEAN\"),\n",
    "        F.count(\"MONTHS_BALANCE\").alias(\"POS_MONTHS_BALANCE_SIZE\"),\n",
    "        F.max(\"SK_DPD\").alias(\"POS_SK_DPD_MAX\"),\n",
    "        F.mean(\"SK_DPD\").alias(\"POS_SK_DPD_MEAN\"),\n",
    "        F.max(\"SK_DPD_DEF\").alias(\"POS_SK_DPD_DEF_MAX\"),\n",
    "        F.mean(\"SK_DPD_DEF\").alias(\"POS_SK_DPD_DEF_MEAN\"),\n",
    "    ]\n",
    "    # cat_features = [c for c in pos.columns if c.startswith(\"NAME_CONTRACT_STATUS_vec\")]\n",
    "    # for cat in cat_features:\n",
    "    #     agg_exprs.append(F.mean(cat).alias(f\"POS_{cat}_MEAN\"))\n",
    "    pos_agg = pos.groupBy(\"SK_ID_CURR\").agg(*agg_exprs)\n",
    "    pos_agg = pos_agg.withColumn(\"POS_COUNT\", F.lit(None))\n",
    "    return pos_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aede06b-e5d9-46a1-9f4a-4ac4d039a6cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bureau Balance (Alternate Version)"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Bureau Balance (Alternate Version)\n",
    "# Purpose: Alternate version of bureau_and_balance, similar to Cell 4, with minor code differences in categorical feature handling and aggregation.\n",
    "# Note: Spark one-hot encoding and categorical feature aggregation are commented out and not used. Numeric-only pipeline for simplicity and performance.\n",
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = spark.table(\"workspace.silver_creditrisk.bureau\")\n",
    "    bb = spark.table(\"workspace.silver_creditrisk.bureau_balance\")\n",
    "    if num_rows:\n",
    "        bureau = bureau.limit(num_rows)\n",
    "        bb = bb.limit(num_rows)\n",
    "    # bb = one_hot_encode(bb, \"STATUS\", \"STATUS\")\n",
    "    # bureau = one_hot_encode(bureau, \"CREDIT_ACTIVE\", \"CREDIT_ACTIVE\")\n",
    "    bb_agg_exprs = [\n",
    "        F.min(\"MONTHS_BALANCE\").alias(\"MONTHS_BALANCE_MIN\"),\n",
    "        F.max(\"MONTHS_BALANCE\").alias(\"MONTHS_BALANCE_MAX\"),\n",
    "        F.count(\"MONTHS_BALANCE\").alias(\"MONTHS_BALANCE_SIZE\"),\n",
    "    ]\n",
    "    bb_agg = bb.groupBy(\"SK_ID_BUREAU\").agg(*bb_agg_exprs)\n",
    "    bureau = bureau.join(bb_agg, on=\"SK_ID_BUREAU\", how=\"left\")\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': [F.min, F.max, F.mean, F.variance],\n",
    "        'CREDIT_DAY_OVERDUE': [F.max, F.mean],\n",
    "        'DAYS_CREDIT_ENDDATE': [F.min, F.max, F.mean],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': [F.mean],\n",
    "        'CNT_CREDIT_PROLONG': [F.sum],\n",
    "        'AMT_CREDIT_SUM': [F.max, F.mean, F.sum],\n",
    "        'AMT_CREDIT_SUM_DEBT': [F.max, F.mean, F.sum],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': [F.mean],\n",
    "        'AMT_CREDIT_SUM_LIMIT': [F.mean, F.sum],\n",
    "        'DAYS_CREDIT_UPDATE': [F.min, F.max, F.mean],\n",
    "        'AMT_ANNUITY': [F.max, F.mean],\n",
    "        'MONTHS_BALANCE_MIN': [F.min],\n",
    "        'MONTHS_BALANCE_MAX': [F.max],\n",
    "        'MONTHS_BALANCE_SIZE': [F.mean, F.sum]\n",
    "    }\n",
    "    agg_exprs = []\n",
    "    for col, funcs in num_aggregations.items():\n",
    "        for func in funcs:\n",
    "            agg_exprs.append(func(col).alias(f\"BURO_{col}_{func.__name__.upper()}\"))\n",
    "    # bureau_cat_features = [c for c in bureau.columns if c.startswith(\"CREDIT_ACTIVE_vec\")]\n",
    "    # for cat in bureau_cat_features:\n",
    "    #     agg_exprs.append(F.mean(cat).alias(f\"BURO_{cat}_MEAN\"))\n",
    "    bureau_agg = bureau.groupBy(\"SK_ID_CURR\").agg(*agg_exprs)\n",
    "    # Minimal fix: Remove active and closed credit aggregations that use CREDIT_ACTIVE_vec[1] and [2]\n",
    "    # active = bureau.filter(F.col(\"CREDIT_ACTIVE_vec\")[1] == 1)\n",
    "    # active_agg_exprs = []\n",
    "    # for col, funcs in num_aggregations.items():\n",
    "    #     for func in funcs:\n",
    "    #         active_agg_exprs.append(func(col).alias(f\"ACT_{col}_{func.__name__.upper()}\"))\n",
    "    # active_agg = active.groupBy(\"SK_ID_CURR\").agg(*active_agg_exprs)\n",
    "    # bureau_agg = bureau_agg.join(active_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    # closed = bureau.filter(F.col(\"CREDIT_ACTIVE_vec\")[2] == 1)\n",
    "    # closed_agg_exprs = []\n",
    "    # for col, funcs in num_aggregations.items():\n",
    "    #     for func in funcs:\n",
    "    #         closed_agg_exprs.append(func(col).alias(f\"CLS_{col}_{func.__name__.upper()}\"))\n",
    "    # closed_agg = closed.groupBy(\"SK_ID_CURR\").agg(*closed_agg_exprs)\n",
    "    # bureau_agg = bureau_agg.join(closed_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    return bureau_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd33982-101a-48d6-bc8a-3f0a245aa5e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Installments Payments Feature Engineering"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Installments Payments Feature Engineering\n",
    "# Purpose: Processes installment payment data, engineers payment and delinquency features, and aggregates by customer ID.\n",
    "# Note: Spark one-hot encoding and categorical aggregations are commented out and not used. Numeric-only pipeline for simplicity and performance.\n",
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    ins = spark.table(\"workspace.silver_creditrisk.installments_payments\")\n",
    "    if num_rows:\n",
    "        ins = ins.limit(num_rows)\n",
    "    # Remove Spark one-hot encoding\n",
    "    # ins = one_hot_encode(ins, \"NAME_CONTRACT_TYPE\", \"NAME_CONTRACT_TYPE\")\n",
    "    ins = ins.withColumn(\"PAYMENT_PERC\", F.try_divide(F.col(\"AMT_PAYMENT\"), F.col(\"AMT_INSTALMENT\")))\n",
    "    ins = ins.withColumn(\"PAYMENT_DIFF\", F.col(\"AMT_INSTALMENT\") - F.col(\"AMT_PAYMENT\"))\n",
    "    ins = ins.withColumn(\"DPD\", F.when(F.col(\"DAYS_ENTRY_PAYMENT\") - F.col(\"DAYS_INSTALMENT\") > 0, F.col(\"DAYS_ENTRY_PAYMENT\") - F.col(\"DAYS_INSTALMENT\")).otherwise(0))\n",
    "    ins = ins.withColumn(\"DBD\", F.when(F.col(\"DAYS_INSTALMENT\") - F.col(\"DAYS_ENTRY_PAYMENT\") > 0, F.col(\"DAYS_INSTALMENT\") - F.col(\"DAYS_ENTRY_PAYMENT\")).otherwise(0))\n",
    "    agg_exprs = [\n",
    "        F.countDistinct(\"NUM_INSTALMENT_VERSION\").alias(\"INS_NUM_INSTALMENT_VERSION_NUNIQUE\"),\n",
    "        F.max(\"DPD\").alias(\"INS_DPD_MAX\"),\n",
    "        F.mean(\"DPD\").alias(\"INS_DPD_MEAN\"),\n",
    "        F.sum(\"DPD\").alias(\"INS_DPD_SUM\"),\n",
    "        F.max(\"DBD\").alias(\"INS_DBD_MAX\"),\n",
    "        F.mean(\"DBD\").alias(\"INS_DBD_MEAN\"),\n",
    "        F.sum(\"DBD\").alias(\"INS_DBD_SUM\"),\n",
    "        F.max(\"PAYMENT_PERC\").alias(\"INS_PAYMENT_PERC_MAX\"),\n",
    "        F.mean(\"PAYMENT_PERC\").alias(\"INS_PAYMENT_PERC_MEAN\"),\n",
    "        F.sum(\"PAYMENT_PERC\").alias(\"INS_PAYMENT_PERC_SUM\"),\n",
    "        F.variance(\"PAYMENT_PERC\").alias(\"INS_PAYMENT_PERC_VAR\"),\n",
    "        F.max(\"PAYMENT_DIFF\").alias(\"INS_PAYMENT_DIFF_MAX\"),\n",
    "        F.mean(\"PAYMENT_DIFF\").alias(\"INS_PAYMENT_DIFF_MEAN\"),\n",
    "        F.sum(\"PAYMENT_DIFF\").alias(\"INS_PAYMENT_DIFF_SUM\"),\n",
    "        F.variance(\"PAYMENT_DIFF\").alias(\"INS_PAYMENT_DIFF_VAR\"),\n",
    "        F.max(\"AMT_INSTALMENT\").alias(\"INS_AMT_INSTALMENT_MAX\"),\n",
    "        F.mean(\"AMT_INSTALMENT\").alias(\"INS_AMT_INSTALMENT_MEAN\"),\n",
    "        F.sum(\"AMT_INSTALMENT\").alias(\"INS_AMT_INSTALMENT_SUM\"),\n",
    "        F.min(\"AMT_PAYMENT\").alias(\"INS_AMT_PAYMENT_MIN\"),\n",
    "        F.max(\"AMT_PAYMENT\").alias(\"INS_AMT_PAYMENT_MAX\"),\n",
    "        F.mean(\"AMT_PAYMENT\").alias(\"INS_AMT_PAYMENT_MEAN\"),\n",
    "        F.sum(\"AMT_PAYMENT\").alias(\"INS_AMT_PAYMENT_SUM\"),\n",
    "        F.max(\"DAYS_ENTRY_PAYMENT\").alias(\"INS_DAYS_ENTRY_PAYMENT_MAX\"),\n",
    "        F.mean(\"DAYS_ENTRY_PAYMENT\").alias(\"INS_DAYS_ENTRY_PAYMENT_MEAN\"),\n",
    "        F.sum(\"DAYS_ENTRY_PAYMENT\").alias(\"INS_DAYS_ENTRY_PAYMENT_SUM\"),\n",
    "    ]\n",
    "    # Remove categorical feature aggregation\n",
    "    # cat_features = [c for c in ins.columns if c.startswith(\"NAME_CONTRACT_TYPE_vec\")]\n",
    "    # for cat in cat_features:\n",
    "    #     agg_exprs.append(F.mean(cat).alias(f\"INS_{cat}_MEAN\"))\n",
    "    ins_agg = ins.groupBy(\"SK_ID_CURR\").agg(*agg_exprs)\n",
    "    ins_agg = ins_agg.withColumn(\"INS_COUNT\", F.lit(None))\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69006b85-b57e-4347-a260-43d88644adbb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "credit_card_balance"
    }
   },
   "outputs": [],
   "source": [
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    cc = spark.table(\"workspace.silver_creditrisk.credit_card_balance\")\n",
    "    if num_rows:\n",
    "        cc = cc.limit(num_rows)\n",
    "    # Remove Spark one-hot encoding\n",
    "    # cc = one_hot_encode(cc, \"NAME_CONTRACT_STATUS\", \"NAME_CONTRACT_STATUS\")\n",
    "    cc = cc.drop(\"SK_ID_PREV\")\n",
    "    # Only aggregate numeric columns\n",
    "    num_cols = [c for c, t in cc.dtypes if c != \"SK_ID_CURR\" and t in [\"int\", \"bigint\", \"double\", \"float\", \"decimal\"]]\n",
    "    agg_exprs = []\n",
    "    for col in num_cols:\n",
    "        agg_exprs.extend([\n",
    "            F.min(col).alias(f\"CC_{col}_MIN\"),\n",
    "            F.max(col).alias(f\"CC_{col}_MAX\"),\n",
    "            F.mean(col).alias(f\"CC_{col}_MEAN\"),\n",
    "            F.sum(col).alias(f\"CC_{col}_SUM\"),\n",
    "            F.variance(col).alias(f\"CC_{col}_VAR\")\n",
    "        ])\n",
    "    cc_agg = cc.groupBy(\"SK_ID_CURR\").agg(*agg_exprs)\n",
    "    cc_count = cc.groupBy(\"SK_ID_CURR\").count().withColumnRenamed(\"count\", \"CC_COUNT\")\n",
    "    cc_agg = cc_agg.join(cc_count, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c18af16-cc03-40b2-87eb-1251d0b2b15e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering Pipeline Orchestration (ML removed)"
    }
   },
   "outputs": [],
   "source": [
    "def main(debug = False):\n",
    "    num_rows = 100000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df count:\", bureau.count())\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df count:\", prev.count())\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df count:\", pos.count())\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df count:\", ins.count())\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df count:\", cc.count())\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    print(\"Final feature set row count:\", df.count())\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with timer(\"Full feature engineering run\"):\n",
    "        final_df = main(debug=False)\n",
    "        # Optionally display or save final_df for downstream modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c57840-94de-4035-a993-0afccfaac0d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Feature Enigneering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
