{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab58be12-f871-40ca-8e13-70192f9fd7a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration (Unity Catalog compatible)"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "# This cell contains all project-wide settings and parameters.\n",
    "# Keep this cell at the top for easy reference and updates.\n",
    "\n",
    "# config.py template for Credit Risk Project\n",
    "\n",
    "# Data paths\n",
    "BASE_PATH = \"/Volumes/workspace/HomeCreditDefaultRisk/creditrisk_data\"\n",
    "TRAIN_FILE = \"application_train.csv\"\n",
    "TEST_FILE = \"application_test.csv\"\n",
    "EXTERNAL_DATA = [\n",
    "    \"bureau.csv\",\n",
    "    \"previous_application.csv\",\n",
    "    # Add other external data files as needed\n",
    "]\n",
    "\n",
    "# Unity Catalog settings\n",
    "CATALOG = \"creditrisk_catalog\"\n",
    "BRONZE_SCHEMA = \"bronze_creditrisk\"\n",
    "SILVER_SCHEMA = \"silver_creditrisk\"\n",
    "GOLD_SCHEMA = \"gold_creditrisk\"\n",
    "TRAIN_TABLE = \"train_dataset\"\n",
    "TEST_TABLE = \"test_dataset\"\n",
    "\n",
    "# Feature engineering\n",
    "SELECTED_FEATURES = [\n",
    "    \"SK_ID_CURR\", \"TARGET\", \"AMT_INCOME_TOTAL\",\n",
    "    # Add more features as needed\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"NAME_CONTRACT_TYPE\", \"CODE_GENDER\",\n",
    "    # Add more categorical features as needed\n",
    "]\n",
    "NUMERICAL_FEATURES = [\n",
    "    \"AMT_CREDIT\", \"AMT_ANNUITY\",\n",
    "    # Add more numerical features as needed\n",
    "]\n",
    "\n",
    "# Model parameters\n",
    "MODEL_TYPE = \"lightgbm\"\n",
    "LGBM_PARAMS = {\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 100,\n",
    "}\n",
    "KFOLDS = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Access control\n",
    "ADMIN_GROUP = \"admins@company.com\"\n",
    "ANALYST_GROUP = \"analysts@company.com\"\n",
    "AIMLDS_GROUP = \"AIMLDS@ds.com\"\n",
    "\n",
    "# Output settings\n",
    "OUTPUT_PATH = \"/dbfs/FileStore/creditrisk/output/\"\n",
    "SUBMISSION_FILE = \"submission.csv\"\n",
    "\n",
    "# Logging\n",
    "LOG_LEVEL = \"INFO\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada01e5f-0011-4e33-8d7b-bc4925603fc5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sanity checks and target distribution"
    }
   },
   "outputs": [],
   "source": [
    "# Sanity checks: one row per SK_ID_CURR\n",
    "print(\"train distinct SK_ID_CURR:\", app_train.select(\"SK_ID_CURR\").distinct().count())\n",
    "print(\"test  distinct SK_ID_CURR:\", app_test.select(\"SK_ID_CURR\").distinct().count())\n",
    "\n",
    "# Target distribution\n",
    "app_train.groupBy(\"TARGET\").count().orderBy(\"TARGET\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea5a9ee-0561-48fd-abd5-bd624785141e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schema Creation and Validation (with catalog creation)"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Schema Creation and Validation\n",
    "# Create Catalog first, then Schemas using config variables\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.`{BRONZE_SCHEMA}`\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SILVER_SCHEMA}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{GOLD_SCHEMA}\")\n",
    "\n",
    "# Validate grain (table must exist first, otherwise this will error)\n",
    "# Uncomment the following lines only after you have written app_train to the target table\n",
    "# st = spark.table(f\"{CATALOG}.{SILVER_SCHEMA}.app_train\")\n",
    "# print(st.count(), st.select(\"SK_ID_CURR\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889f9fa0-e469-4d90-83c3-724040a303d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Access Control and Privileges (Python, Unity Catalog compatible, minimal fix)"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Access Control and Privileges\n",
    "# Use config variables for catalog, schemas, and groups\n",
    "\n",
    "# Ensure catalog exists\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "# Switch to catalog\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# Create schemas\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{CATALOG}`.`{BRONZE_SCHEMA}`\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{CATALOG}`.`{SILVER_SCHEMA}`\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{CATALOG}`.`{GOLD_SCHEMA}`\")\n",
    "\n",
    "# Ownership\n",
    "spark.sql(f\"ALTER SCHEMA `{CATALOG}`.`{BRONZE_SCHEMA}` OWNER TO `{ADMIN_GROUP}`\")\n",
    "spark.sql(f\"ALTER SCHEMA `{CATALOG}`.`{SILVER_SCHEMA}` OWNER TO `{ADMIN_GROUP}`\")\n",
    "spark.sql(f\"ALTER SCHEMA `{CATALOG}`.`{GOLD_SCHEMA}` OWNER TO `{ADMIN_GROUP}`\")\n",
    "\n",
    "# Privileges for bronze\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{BRONZE_SCHEMA}` TO `{ADMIN_GROUP}`\")\n",
    "spark.sql(f\"REVOKE ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{BRONZE_SCHEMA}` FROM `{ANALYST_GROUP}`\")\n",
    "spark.sql(f\"REVOKE ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{BRONZE_SCHEMA}` FROM `{AIMLDS_GROUP}`\")\n",
    "\n",
    "# Privileges for silver\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{SILVER_SCHEMA}` TO `{ADMIN_GROUP}`\")\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{SILVER_SCHEMA}` TO `{AIMLDS_GROUP}`\")\n",
    "spark.sql(f\"REVOKE ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{SILVER_SCHEMA}` FROM `{ANALYST_GROUP}`\")\n",
    "\n",
    "# Privileges for gold\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{GOLD_SCHEMA}` TO `{ADMIN_GROUP}`\")\n",
    "spark.sql(f\"GRANT SELECT ON SCHEMA `{CATALOG}`.`{GOLD_SCHEMA}` TO `{ANALYST_GROUP}`\")\n",
    "spark.sql(f\"GRANT SELECT ON SCHEMA `{CATALOG}`.`{GOLD_SCHEMA}` TO `{AIMLDS_GROUP}`\")\n",
    "\n",
    "# Table-level grants (uncomment after table creation)\n",
    "# spark.sql(f\"GRANT SELECT ON TABLE `{CATALOG}`.`{GOLD_SCHEMA}`.`{TRAIN_TABLE}` TO `{ANALYST_GROUP}`\")\n",
    "\n",
    "# Show grants\n",
    "spark.sql(f\"SHOW GRANTS ON SCHEMA `{CATALOG}`.`{BRONZE_SCHEMA}`\")\n",
    "spark.sql(f\"SHOW GRANTS ON SCHEMA `{CATALOG}`.`{SILVER_SCHEMA}`\")\n",
    "spark.sql(f\"SHOW GRANTS ON SCHEMA `{CATALOG}`.`{GOLD_SCHEMA}`\")\n",
    "# spark.sql(f\"SHOW GRANTS ON TABLE `{CATALOG}`.`{GOLD_SCHEMA}`.`{TRAIN_TABLE}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea58489-2b83-4de1-8161-d6b0067933bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ownership using config"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"ALTER SCHEMA `{CATALOG}`.`{BRONZE_SCHEMA}` OWNER TO `{ADMIN_GROUP}`\")\n",
    "spark.sql(f\"ALTER SCHEMA `{CATALOG}`.`{SILVER_SCHEMA}` OWNER TO `{ADMIN_GROUP}`\")\n",
    "spark.sql(f\"ALTER SCHEMA `{CATALOG}`.`{GOLD_SCHEMA}` OWNER TO `{ADMIN_GROUP}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16da89a4-ec48-4b87-b111-266b17ddd494",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "BRONZE SCHEMA privileges using config (Python, Unity Catalog compatible)"
    }
   },
   "outputs": [],
   "source": [
    "# BRONZE SCHEMA privileges using config\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{BRONZE_SCHEMA}` TO `{ADMIN_GROUP}`\")\n",
    "spark.sql(f\"REVOKE ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{BRONZE_SCHEMA}` FROM `{ANALYST_GROUP}`\")\n",
    "spark.sql(f\"REVOKE ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{BRONZE_SCHEMA}` FROM `{AIMLDS_GROUP}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe01216-8214-4ee0-9cf3-29cccd6e3714",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver Schema privileges using config (Python, Unity Catalog compatible)"
    }
   },
   "outputs": [],
   "source": [
    "# Silver Schema privileges using config\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{SILVER_SCHEMA}` TO `{ADMIN_GROUP}`\")\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{SILVER_SCHEMA}` TO `{AIMLDS_GROUP}`\")\n",
    "spark.sql(f\"REVOKE ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{SILVER_SCHEMA}` FROM `{ANALYST_GROUP}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c1e4be3-92af-4090-93bc-046a1f81f35d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gold Schema privileges using config (Python, Unity Catalog compatible)"
    }
   },
   "outputs": [],
   "source": [
    "# Gold Schema privileges using config\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON SCHEMA `{CATALOG}`.`{GOLD_SCHEMA}` TO `{ADMIN_GROUP}`\")\n",
    "spark.sql(f\"GRANT SELECT ON SCHEMA `{CATALOG}`.`{GOLD_SCHEMA}` TO `{ANALYST_GROUP}`\")\n",
    "spark.sql(f\"GRANT SELECT ON SCHEMA `{CATALOG}`.`{GOLD_SCHEMA}` TO `{AIMLDS_GROUP}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cff0837-b277-4af7-9966-e6e50cdab35e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Table level access using config (minimal fix: ensure table exists)"
    }
   },
   "outputs": [],
   "source": [
    "# Minimal fix: Ensure table exists before granting access\n",
    "# If the table does not exist, create it from app_train\n",
    "if not spark.catalog.tableExists(f\"{CATALOG}.{GOLD_SCHEMA}.{TRAIN_TABLE}\"):\n",
    "    app_train.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{GOLD_SCHEMA}.{TRAIN_TABLE}\")\n",
    "\n",
    "# Now grant access\n",
    "spark.sql(f\"GRANT SELECT ON TABLE `{CATALOG}`.`{GOLD_SCHEMA}`.`{TRAIN_TABLE}` TO `{ANALYST_GROUP}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2255131d-3220-40b2-bcd0-5c416bb9b109",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Show grants using config (Python, Unity Catalog compatible)"
    }
   },
   "outputs": [],
   "source": [
    "# Show grants using config\n",
    "spark.sql(f\"SHOW GRANTS ON SCHEMA `{CATALOG}`.`{BRONZE_SCHEMA}`\")\n",
    "spark.sql(f\"SHOW GRANTS ON SCHEMA `{CATALOG}`.`{SILVER_SCHEMA}`\")\n",
    "spark.sql(f\"SHOW GRANTS ON SCHEMA `{CATALOG}`.`{GOLD_SCHEMA}`\")\n",
    "spark.sql(f\"SHOW GRANTS ON TABLE `{CATALOG}`.`{GOLD_SCHEMA}`.`{TRAIN_TABLE}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1339f331-9f3a-4f85-88b7-3ca00b71b490",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Documentation/Notes"
    }
   },
   "source": [
    "### 7. Documentation/Notes\n",
    "#\n",
    "### The workspace was running Unity Catalog metastore privilege version 1.0, which does not support USAGE privileges. Governance was implemented using schema- and table-level ALL and SELECT grants, with column-level restrictions for sensitive credit attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "643751e7-1f45-4923-96c9-ac802462138b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analysis and Reporting"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Analysis and Reporting\n",
    "# Target distribution\n",
    "# %sql\n",
    "# SELECT\n",
    "#   TARGET,\n",
    "#   COUNT(*) AS customers,\n",
    "#   ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS pct\n",
    "# FROM {GOLD_SCHEMA}.{TRAIN_TABLE}\n",
    "# GROUP BY TARGET\n",
    "# ORDER BY TARGET;\n",
    "\n",
    "# Risk flag prevalence\n",
    "# %sql\n",
    "# SELECT\n",
    "#   ROUND(100.0 * AVG(severe_dpd_flag), 2)       AS severe_dpd_pct,\n",
    "#   ROUND(100.0 * AVG(frequent_late_payer_flag), 2) AS frequent_late_payer_pct,\n",
    "#   ROUND(100.0 * AVG(high_bureau_risk_flag), 2) AS high_bureau_risk_pct\n",
    "# FROM {GOLD_SCHEMA}.{TRAIN_TABLE};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460bc603-0056-4ca5-a113-41cf44c55e5c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Risk flag prevalence"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT\n",
    "#   ROUND(100.0 * AVG(severe_dpd_flag), 2)       AS severe_dpd_pct,\n",
    "#   ROUND(100.0 * AVG(frequent_late_payer_flag), 2) AS frequent_late_payer_pct,\n",
    "#   ROUND(100.0 * AVG(high_bureau_risk_flag), 2) AS high_bureau_risk_pct\n",
    "# FROM workspace.gold_creditrisk.train_dataset_b;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d538a98c-a0a8-4ccd-93aa-8e0a7f0c0676",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature Engineering and Utility Functions"
    }
   },
   "outputs": [],
   "source": [
    "# # 5. Feature Engineering and Utility Functions\n",
    "# def missing_data(df):\n",
    "#     total_count = df.count()\n",
    "#     null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0]\n",
    "#     result = spark.createDataFrame(\n",
    "#         [(c, null_counts[c], (null_counts[c] / total_count * 100) if total_count > 0 else 0) for c in df.columns],\n",
    "#         [\"Column\", \"Total\", \"Percent\"]\n",
    "#     )\n",
    "#     return result.orderBy(F.col(\"Total\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade312b8-458f-45b5-b38c-55f5e42e49d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Loading and Initial Exploration"
    }
   },
   "outputs": [],
   "source": [
    "# # 2. Data Loading and Initial Exploration\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# app_train = spark.read.csv(f\"{BASE_PATH}/{TRAIN_FILE}\", header=True, inferSchema=True)\n",
    "# app_test = spark.read.csv(f\"{BASE_PATH}/{TEST_FILE}\", header=True, inferSchema=True)\n",
    "\n",
    "# print(\"train rows:\", app_train.count())\n",
    "# print(\"test rows:\", app_test.count())\n",
    "\n",
    "# app_train.select(\"SK_ID_CURR\", \"TARGET\").show(5)\n",
    "\n",
    "# # Sanity checks: one row per SK_ID_CURR\n",
    "# print(\"train distinct SK_ID_CURR:\", app_train.select(\"SK_ID_CURR\").distinct().count())\n",
    "# print(\"test  distinct SK_ID_CURR:\", app_test.select(\"SK_ID_CURR\").distinct().count())\n",
    "\n",
    "# # Target distribution\n",
    "# app_train.groupBy(\"TARGET\").count().orderBy(\"TARGET\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52da9ab1-8b9c-49a3-8415-b6364014d4c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b924e540-31a2-4f57-a589-67111a1b5a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b20860a4-834c-4518-aa14-6b9df899ecb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CreditRisk_Config",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
